{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_prompt = \"\"\"你精通python代码的静态检查相关的工具和原理，假设我拿到了python工具bandit扫描的结果，结果是json格式的，我需要分析这个结果从而确认是否误报，是否需要高优先级修复。\n",
    "首先，请根据给出的json数据，帮我找出总共有多少高危（Severity = HIGH）、中危（Severity = MEDIUM）、低危（Severity = LOW）的漏洞。\n",
    "\n",
    "json数据：\n",
    "```json\n",
    "{json_data}\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ssdlc 安全检查结果分析器**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "根据你提供的JSON数据，我们可以分析出不同严重性级别的漏洞数量。这些信息在`metrics`部分的每个文件条目以及总计（`_totals`）中都有记录。具体来说，我们关注的是`SEVERITY.HIGH`, `SEVERITY.MEDIUM`, 和 `SEVERITY.LOW`这三个字段。\n",
      "\n",
      "以下是计算结果：\n",
      "\n",
      "1. **高危 (Severity = HIGH)**:\n",
      "   - 从`_totals`中可以看到，总共有1个高危漏洞 (`SEVERITY.HIGH`: 1)。\n",
      "   - 具体到文件层面，`./bench/common/system.py` 文件有1个高危漏洞。\n",
      "\n",
      "2. **中危 (Severity = MEDIUM)**:\n",
      "   - 从`_totals`中可以看到，总共有0个中危漏洞 (`SEVERITY.MEDIUM`: 0)。\n",
      "   - 没有任何文件报告中危漏洞。\n",
      "\n",
      "3. **低危 (Severity = LOW)**:\n",
      "   - 从`_totals`中可以看到，总共有3个低危漏洞 (`SEVERITY.LOW`: 3)。\n",
      "   - 具体到文件层面：\n",
      "     - `./bench/common/system.py` 文件有1个低危漏洞。\n",
      "     - `./bench/controller/benchmark.py` 文件有2个低危漏洞。\n",
      "\n",
      "总结如下：\n",
      "- 高危漏洞：1个\n",
      "- 中危漏洞：0个\n",
      "- 低危漏洞：3个\n",
      "\n",
      "接下来，你可以根据这些信息来评估每个漏洞的具体情况，并决定是否需要高优先级修复。对于高危漏洞，通常建议立即处理；而对于低危漏洞，可以根据具体情况和项目需求来决定修复的优先级。\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv(\"./env/.env\"))\n",
    "\n",
    "import dashscope\n",
    "from http import HTTPStatus\n",
    "from pprint import pprint\n",
    "import json\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_community.chat_models import ChatTongyi\n",
    "\n",
    "llm_model = \"qwen-max\"\n",
    "\n",
    "llm = ChatTongyi(temperature=1.0, model=llm_model)\n",
    "\n",
    "json_analysis_prompt = ChatPromptTemplate.from_template(json_prompt)\n",
    "\n",
    "with open(\"./data/result.json\", \"r\") as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "response = llm.invoke(json_analysis_prompt.format_messages(json_data=json_data))\n",
    "\n",
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
