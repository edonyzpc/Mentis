{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KeyError exception handling in Python\n",
    "When calling Qwen API with an invalid key, you will receive a KeyError exception(request). Here is the exception details:\n",
    "\n",
    "```python\n",
    "---------------------------------------------------------------------------\n",
    "KeyError                                  Traceback (most recent call last)\n",
    "Cell In[33], line 60\n",
    "     58     source_codes = [open(os.path.join(\"./data/\", \"sysom-3.3.0\", code_path), \"r\").read() for code_path in calling_source_code_paths[1:]]\n",
    "     59     calling_source_code = \"\\n\".join(source_codes)\n",
    "---> 60 response = llm.invoke(\n",
    "     61     parse_high_risk_call_graph_prompt.format_messages(\n",
    "     62         high_risk=high_risk,\n",
    "     63         issue_source_code=issue_source_code, \n",
    "     64         calling_source_code=calling_source_code\n",
    "     65     )\n",
    "     66 )\n",
    "     68 high_risk_call_graph = response.content\n",
    "     69 high_risk_call_graphs.append(high_risk_call_graph)\n",
    "\n",
    "File ~/code/Mentis/venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:286, in BaseChatModel.invoke(self, input, config, stop, **kwargs)\n",
    "    275 def invoke(\n",
    "    276     self,\n",
    "    277     input: LanguageModelInput,\n",
    "   (...)\n",
    "    281     **kwargs: Any,\n",
    "    282 ) -> BaseMessage:\n",
    "    283     config = ensure_config(config)\n",
    "    284     return cast(\n",
    "    285         ChatGeneration,\n",
    "--> 286         self.generate_prompt(\n",
    "    287             [self._convert_input(input)],\n",
    "    288             stop=stop,\n",
    "    289             callbacks=config.get(\"callbacks\"),\n",
    "    290             tags=config.get(\"tags\"),\n",
    "    291             metadata=config.get(\"metadata\"),\n",
    "    292             run_name=config.get(\"run_name\"),\n",
    "    293             run_id=config.pop(\"run_id\", None),\n",
    "    294             **kwargs,\n",
    "    295         ).generations[0][0],\n",
    "    296     ).message\n",
    "\n",
    "File ~/code/Mentis/venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:786, in BaseChatModel.generate_prompt(self, prompts, stop, callbacks, **kwargs)\n",
    "    778 def generate_prompt(\n",
    "    779     self,\n",
    "    780     prompts: List[PromptValue],\n",
    "   (...)\n",
    "    783     **kwargs: Any,\n",
    "    784 ) -> LLMResult:\n",
    "    785     prompt_messages = [p.to_messages() for p in prompts]\n",
    "--> 786     return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
    "\n",
    "File ~/code/Mentis/venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:643, in BaseChatModel.generate(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\n",
    "    641         if run_managers:\n",
    "    642             run_managers[i].on_llm_error(e, response=LLMResult(generations=[]))\n",
    "--> 643         raise e\n",
    "    644 flattened_outputs = [\n",
    "    645     LLMResult(generations=[res.generations], llm_output=res.llm_output)  # type: ignore[list-item]\n",
    "    646     for res in results\n",
    "    647 ]\n",
    "    648 llm_output = self._combine_llm_outputs([res.llm_output for res in results])\n",
    "\n",
    "File ~/code/Mentis/venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:633, in BaseChatModel.generate(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\n",
    "    630 for i, m in enumerate(messages):\n",
    "    631     try:\n",
    "    632         results.append(\n",
    "--> 633             self._generate_with_cache(\n",
    "    634                 m,\n",
    "    635                 stop=stop,\n",
    "    636                 run_manager=run_managers[i] if run_managers else None,\n",
    "    637                 **kwargs,\n",
    "    638             )\n",
    "    639         )\n",
    "    640     except BaseException as e:\n",
    "    641         if run_managers:\n",
    "\n",
    "File ~/code/Mentis/venv/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py:855, in BaseChatModel._generate_with_cache(self, messages, stop, run_manager, **kwargs)\n",
    "    853 else:\n",
    "    854     if inspect.signature(self._generate).parameters.get(\"run_manager\"):\n",
    "--> 855         result = self._generate(\n",
    "    856             messages, stop=stop, run_manager=run_manager, **kwargs\n",
    "    857         )\n",
    "    858     else:\n",
    "    859         result = self._generate(messages, stop=stop, **kwargs)\n",
    "\n",
    "File ~/code/Mentis/venv/lib/python3.10/site-packages/langchain_community/chat_models/tongyi.py:647, in ChatTongyi._generate(self, messages, stop, run_manager, **kwargs)\n",
    "    643 else:\n",
    "    644     params: Dict[str, Any] = self._invocation_params(\n",
    "    645         messages=messages, stop=stop, **kwargs\n",
    "    646     )\n",
    "--> 647     resp = self.completion_with_retry(**params)\n",
    "    648     generations.append(\n",
    "    649         ChatGeneration(**self._chat_generation_from_qwen_resp(resp))\n",
    "    650     )\n",
    "    651 return ChatResult(\n",
    "    652     generations=generations,\n",
    "    653     llm_output={\n",
    "    654         \"model_name\": self.model_name,\n",
    "    655     },\n",
    "    656 )\n",
    "\n",
    "File ~/code/Mentis/venv/lib/python3.10/site-packages/langchain_community/chat_models/tongyi.py:533, in ChatTongyi.completion_with_retry(self, **kwargs)\n",
    "    530     resp = self.client.call(**_kwargs)\n",
    "    531     return check_response(resp)\n",
    "--> 533 return _completion_with_retry(**kwargs)\n",
    "\n",
    "File ~/code/Mentis/venv/lib/python3.10/site-packages/tenacity/__init__.py:330, in BaseRetrying.wraps.<locals>.wrapped_f(*args, **kw)\n",
    "    326 @functools.wraps(\n",
    "    327     f, functools.WRAPPER_ASSIGNMENTS + (\"__defaults__\", \"__kwdefaults__\")\n",
    "    328 )\n",
    "    329 def wrapped_f(*args: t.Any, **kw: t.Any) -> t.Any:\n",
    "--> 330     return self(f, *args, **kw)\n",
    "\n",
    "File ~/code/Mentis/venv/lib/python3.10/site-packages/tenacity/__init__.py:467, in Retrying.__call__(self, fn, *args, **kwargs)\n",
    "    465 retry_state = RetryCallState(retry_object=self, fn=fn, args=args, kwargs=kwargs)\n",
    "    466 while True:\n",
    "--> 467     do = self.iter(retry_state=retry_state)\n",
    "    468     if isinstance(do, DoAttempt):\n",
    "    469         try:\n",
    "\n",
    "File ~/code/Mentis/venv/lib/python3.10/site-packages/tenacity/__init__.py:368, in BaseRetrying.iter(self, retry_state)\n",
    "    366 result = None\n",
    "    367 for action in self.iter_state.actions:\n",
    "--> 368     result = action(retry_state)\n",
    "    369 return result\n",
    "\n",
    "File ~/code/Mentis/venv/lib/python3.10/site-packages/tenacity/__init__.py:390, in BaseRetrying._post_retry_check_actions.<locals>.<lambda>(rs)\n",
    "    388 def _post_retry_check_actions(self, retry_state: \"RetryCallState\") -> None:\n",
    "    389     if not (self.iter_state.is_explicit_retry or self.iter_state.retry_run_result):\n",
    "--> 390         self._add_action_func(lambda rs: rs.outcome.result())\n",
    "    391         return\n",
    "    393     if self.after is not None:\n",
    "\n",
    "File /opt/homebrew/Cellar/python@3.10/3.10.10_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/concurrent/futures/_base.py:451, in Future.result(self, timeout)\n",
    "    449     raise CancelledError()\n",
    "    450 elif self._state == FINISHED:\n",
    "--> 451     return self.__get_result()\n",
    "    453 self._condition.wait(timeout)\n",
    "    455 if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
    "\n",
    "File /opt/homebrew/Cellar/python@3.10/3.10.10_1/Frameworks/Python.framework/Versions/3.10/lib/python3.10/concurrent/futures/_base.py:403, in Future.__get_result(self)\n",
    "    401 if self._exception:\n",
    "    402     try:\n",
    "--> 403         raise self._exception\n",
    "    404     finally:\n",
    "    405         # Break a reference cycle with the exception in self._exception\n",
    "    406         self = None\n",
    "\n",
    "File ~/code/Mentis/venv/lib/python3.10/site-packages/tenacity/__init__.py:470, in Retrying.__call__(self, fn, *args, **kwargs)\n",
    "    468 if isinstance(do, DoAttempt):\n",
    "    469     try:\n",
    "--> 470         result = fn(*args, **kwargs)\n",
    "    471     except BaseException:  # noqa: B902\n",
    "    472         retry_state.set_exception(sys.exc_info())  # type: ignore[arg-type]\n",
    "\n",
    "File ~/code/Mentis/venv/lib/python3.10/site-packages/langchain_community/chat_models/tongyi.py:531, in ChatTongyi.completion_with_retry.<locals>._completion_with_retry(**_kwargs)\n",
    "    528 @retry_decorator\n",
    "    529 def _completion_with_retry(**_kwargs: Any) -> Any:\n",
    "    530     resp = self.client.call(**_kwargs)\n",
    "--> 531     return check_response(resp)\n",
    "\n",
    "File ~/code/Mentis/venv/lib/python3.10/site-packages/langchain_community/llms/tongyi.py:66, in check_response(resp)\n",
    "     61     raise ValueError(\n",
    "     62         f\"status_code: {resp['status_code']} \\n \"\n",
    "     63         f\"code: {resp['code']} \\n message: {resp['message']}\"\n",
    "     64     )\n",
    "     65 else:\n",
    "---> 66     raise HTTPError(\n",
    "     67         f\"HTTP error occurred: status_code: {resp['status_code']} \\n \"\n",
    "     68         f\"code: {resp['code']} \\n message: {resp['message']}\",\n",
    "     69         response=resp,\n",
    "     70     )\n",
    "\n",
    "File ~/code/Mentis/venv/lib/python3.10/site-packages/requests/exceptions.py:22, in RequestException.__init__(self, *args, **kwargs)\n",
    "     20 self.response = response\n",
    "     21 self.request = kwargs.pop(\"request\", None)\n",
    "---> 22 if response is not None and not self.request and hasattr(response, \"request\"):\n",
    "     23     self.request = self.response.request\n",
    "     24 super().__init__(*args, **kwargs)\n",
    "\n",
    "File ~/code/Mentis/venv/lib/python3.10/site-packages/dashscope/api_entities/dashscope_response.py:59, in DictMixin.__getattr__(self, attr)\n",
    "     58 def __getattr__(self, attr):\n",
    "---> 59     return self[attr]\n",
    "\n",
    "File ~/code/Mentis/venv/lib/python3.10/site-packages/dashscope/api_entities/dashscope_response.py:15, in DictMixin.__getitem__(self, key)\n",
    "     14 def __getitem__(self, key):\n",
    "---> 15     return super().__getitem__(key)\n",
    "\n",
    "KeyError: 'request'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key d not found\n"
     ]
    }
   ],
   "source": [
    "a = {'a':1, 'b':2, 'c':3}\n",
    "try:\n",
    "    print(a['d'])\n",
    "except KeyError as e:\n",
    "    if e.args[0] == 'd':\n",
    "        print('key d not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
